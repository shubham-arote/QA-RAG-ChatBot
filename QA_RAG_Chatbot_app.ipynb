{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM7sHKAI3qCXFlGLy/SMMWc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubham-arote/QA-RAG-ChatBot/blob/main/QA_RAG_Chatbot_app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install langchain==0.1.12\n",
        "!pip install langchain-community==0.0.29\n",
        "!pip install streamlit==1.32.2\n",
        "!pip install PyMuPDF==1.24.0\n",
        "!pip install chromadb==0.4.24\n",
        "!pip install pyngrok==7.1.5\n",
        "!pip install langchain-google-genai==0.0.11\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ],
      "metadata": {
        "id": "WrFF1G_rLXG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
      ],
      "metadata": {
        "id": "jAbAm6zeh0fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF==1.24.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bys_5wIeIcVB",
        "outputId": "73532d34-a2a0-46de-b174-ff5c3b9d866a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyMuPDF==1.24.0 in /usr/local/lib/python3.10/dist-packages (1.24.0)\n",
            "Requirement already satisfied: PyMuPDFb==1.24.0 in /usr/local/lib/python3.10/dist-packages (from PyMuPDF==1.24.0) (1.24.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import yaml\n",
        "#with open('gemini_credentials.yml','r') as file:\n",
        " # api_cred = yaml.safe_load(file)"
      ],
      "metadata": {
        "id": "CLp3yTZIxL60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#api_cred.keys()"
      ],
      "metadata": {
        "id": "doijfuhAxw-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key= \"AIzaSyBP_cpkLKd10bz1fGutvVsYGWDUhRX6hZ0\""
      ],
      "metadata": {
        "id": "hiUb51OnNS3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['GOOGLE_API_KEY'] = api_key"
      ],
      "metadata": {
        "id": "n40OjINjKWso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%writefile app.py\n",
        "from collections.abc import Container\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
        "from langchain_core.callbacks.base import BaseCallbackHandler\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores.chroma import Chroma\n",
        "from operator import itemgetter\n",
        "import streamlit as st\n",
        "import tempfile\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Customize initial landing page\n",
        "st.set_page_config(page_title =\"File QA Chatbot\")\n",
        "st.title(\"Welcome to File QA RAG Chatbot\")\n",
        "\n",
        "@st.cache_resource(ttl =\"1h\")\n",
        "# Takes uploaded PDFs, creates documents chunks, computer embeddings\n",
        "# Stores documents chunks and embeddings in Vector DB\n",
        "# Returns a retreiver which can look up the Vector DB\n",
        "# to return documents based on user input\n",
        "# Stores this in cashe\n",
        "\n",
        "def configure_retriever(uploaded_files):\n",
        "  # Read documents\n",
        "  docs =[]\n",
        "  temp_dir = tempfile.TemporaryDirectory()\n",
        "  for file in uploaded_files:\n",
        "    temp_filepath = os.path.join(temp_dir.name, file.name)\n",
        "    with open(temp_filepath, \"wb\") as f:\n",
        "      f.write(file.getvalue())\n",
        "    loader = PyMuPDFLoader(temp_filepath)\n",
        "    docs.extend(loader.load())\n",
        "\n",
        "# Split into documents chunks\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1500, chunk_overlap=200)\n",
        "  doc_chunks = text_splitter.split_documents(docs)\n",
        "\n",
        "#Create document embeddings and store in vectort DB\n",
        "\n",
        "  embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "  vectordb = Chroma.from_documents(doc_chunks,embeddings)\n",
        "\n",
        "#define retreiver object\n",
        "  retriever = vectordb.as_retriever()\n",
        "  return retriever\n",
        "\n",
        "  #Manages live updates to a stramlit app' display by appending new text tokens\n",
        "  # to an existing text stream and rendering the updated text in Markdown\n",
        "class StreamHandler(BaseCallbackHandler):\n",
        "  def __init__(self, container, initial_text= \"\"):\n",
        "    self.container= container\n",
        "    self.text = initial_text\n",
        "\n",
        "  def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
        "    self.text += token\n",
        "    self.container.markdown(self.text)\n",
        "\n",
        "\n",
        "#Creates UI element  to accept PDF uploads\n",
        "uploaded_files = st.sidebar.file_uploader(\n",
        "  label=\"Upload PDF files\", type=[\"pdf\"],\n",
        "  accept_multiple_files=True\n",
        ")\n",
        "if not uploaded_files:\n",
        "  st.info(\"Please upload PDF documents to continue.\")\n",
        "  st.stop()\n",
        "\n",
        "#Create retriever object based on uploaded PDFs\n",
        "retriever = configure_retriever(uploaded_files)\n",
        "\n",
        "#Load a connections to ChatGPT LLM\n",
        "chatgpt = ChatGoogleGenerativeAI(model=\"gemini-pro\", convert_system_message_to_human=True,\n",
        "                                 streaming=True)\n",
        "\n",
        "#Create  a prmompt template for QA RAG System\n",
        "\n",
        "qa_template = \"\"\"\n",
        "             Use only the following pieces of context to answer the question at the end,\n",
        "             If you don't know the answer, just say that you don't know,\n",
        "             don't try to make up an answer. Keep the answer as concise as possible.\n",
        "\n",
        "             {contxt}\n",
        "\n",
        "             Question :{Question}\n",
        "             \"\"\"\n",
        "qa_prompt = ChatPromptTemplate.from_template(qa_template)\n",
        "\n",
        "def format_docs(docs):\n",
        "  return \"\\n\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "#Create a QA RAG System Chain\n",
        "\n",
        "qa_rag_chain =(\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") # based on user question impoet context docs\n",
        "          |\n",
        "        retriever\n",
        "          |\n",
        "        format_docs,\n",
        "        \"question\": itemgetter(\"question\") #user question\n",
        "    }\n",
        "          |\n",
        "        qa_prompt\n",
        "          |\n",
        "        chatgpt\n",
        ")\n",
        "\n",
        "# Store conversation history in Stramlit session state\n",
        "\n",
        "streamlit_msg_history = StreamlitChatMessageHistory(key= \"langchain_messages\")\n",
        "\n",
        "# Shows the first message when app starts\n",
        "if len(streamlit_msg_history.messages) == 0:\n",
        "  streamlit_msg_history.add_ai_message(\"Please ask your question?\")\n",
        "\n",
        "for msg in streamlit_msg_history.messages:\n",
        "  st.chat_message(msg.type).write(msg.content)\n",
        "\n",
        "#Callback handle which does some post-processing on the LLM response\n",
        "# Used to post the top 3 document sources used by the LLM in RAG response\n",
        "\n",
        "class PostMessageHandler(BaseCallbackHandler):\n",
        "  def __init__(self, msg: st.write):\n",
        "    BaseCallbackHandler.__init__(self)\n",
        "    self.msg = msg\n",
        "    self.sources = []\n",
        "\n",
        "  def on_retriever_end(self, documents, *,run_id, parent_run_id, **kwargs):\n",
        "    source_ids = []\n",
        "\n",
        "    for d in documents:\n",
        "      metadata ={\n",
        "          \"source\": d.metadata[\"source\"],\n",
        "          \"page\": d.metadata[\"page\"],\n",
        "          \"content\": d.page_content[:200]\n",
        "      }\n",
        "      idx= (metadata[\"source\"], metadata[\"page\"])\n",
        "      if idx not in source_ids: # store unique source documents\n",
        "          source_ids.append(idx)\n",
        "          self.sources.append(metadata)\n",
        "\n",
        "  def on_llm_end(self, response, *, run_id, parent_run_id, **kwargs):\n",
        "    if len(self.sources):\n",
        "      st.markdown(\"__Sources:__\"+\"\\n\")\n",
        "      st.dataframe(data=pd.DataFrame(self.sources[:3]),\n",
        "                   width=1000) # Top 3 sources\n",
        "\n",
        "#If user inputs a new prompt, display it in and show the response\n",
        "if user_prompt := st.chat_input():\n",
        "  st.chat_message(\"human\").write(user_prompt)\n",
        "  # This is where response from the LLM is shown\n",
        "  with st.chat_message(\"ai\"):\n",
        "    #Initializing an empty data stream\n",
        "    stream_handler = StreamHandler(st.empty())\n",
        "    # UI element to write RAG sources after LLM response\n",
        "    source_container = st.write(\"\")\n",
        "    pm_handler = PostMessageHandler(source_container)\n",
        "    config= {\"call backs\": [stream_handler, pm_handler]}\n",
        "    # Get LLM response\n",
        "    response = qa_rag_chain.invoke({\"question\": user_prompt},\n",
        "                                    config)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbbdcXuxzlUp",
        "outputId": "a843dbe8-3241-465f-d09b-4003e8b924c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py --server.port=8888 &>./logs.txt &"
      ],
      "metadata": {
        "id": "x8JLwqQdVZbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import yaml\n",
        "\n",
        "# Terminate open tunnels if exist\n",
        "ngrok.kill()\n",
        "\n",
        "#setting the authtoken\n",
        "# Get your authtoken from 'ngrok_credentials.yml' file\n",
        "\n",
        "with open('ngrok_credentials.yml') as file:\n",
        "  NGROK_AUTH_TOKEN = yaml.safe_load(file)\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN['ngrok_key'])\n",
        "\n",
        "# Open an HTTPs tunnel on port XXXX which you get from your 'logs.txt' file\n",
        "ngrok_tunnel = ngrok.connect(8888)\n",
        "print(\"streamlit App:\", ngrok_tunnel.public_url)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liLjTtCbaLDs",
        "outputId": "2dd03e3b-09c8-4901-905b-a8eaa26ff4ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "streamlit App: https://8dd1-34-106-166-1.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lsof -i :8888"
      ],
      "metadata": {
        "id": "GbszrMQkbLoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kill -9 6297"
      ],
      "metadata": {
        "id": "s9nPDysRSFYP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}